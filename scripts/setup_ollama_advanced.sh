#!/bin/bash

# Advanced Ollama Setup Script
# This script automatically detects system architecture and configures Ollama accordingly

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Default configuration
OLLAMA_PORT=${OLLAMA_PORT:-11434}
OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1:8b}
OLLAMA_TARGET_OS=${OLLAMA_TARGET_OS:-linux}

# Function to detect system architecture
detect_architecture() {
    local arch=$(uname -m)
    case $arch in
        x86_64|amd64)
            echo "amd64"
            ;;
        aarch64|arm64)
            echo "arm64"
            ;;
        armv7l)
            echo "arm"
            ;;
        *)
            echo "amd64"  # Default fallback
            ;;
    esac
}

# Function to detect OS
detect_os() {
    local os=$(uname -s | tr '[:upper:]' '[:lower:]')
    case $os in
        darwin)
            echo "darwin"
            ;;
        linux)
            echo "linux"
            ;;
        *)
            echo "linux"  # Default fallback
            ;;
    esac
}

# Function to check for NVIDIA GPU
check_nvidia_gpu() {
    if command -v nvidia-smi > /dev/null 2>&1; then
        nvidia-smi --query-gpu=name --format=csv,noheader,nounits > /dev/null 2>&1
        return $?
    fi
    return 1
}

# Function to check for Apple Silicon
check_apple_silicon() {
    if [[ $(uname -s) == "Darwin" ]] && [[ $(uname -m) == "arm64" ]]; then
        return 0
    fi
    return 1
}

# Function to determine optimal configuration
determine_configuration() {
    local detected_arch=$(detect_architecture)
    local detected_os=$(detect_os)
    local has_nvidia_gpu=false
    local is_apple_silicon=false
    
    echo -e "${BLUE}Detecting system configuration...${NC}"
    echo "  Architecture: $detected_arch"
    echo "  OS: $detected_os"
    
    if check_nvidia_gpu; then
        has_nvidia_gpu=true
        echo "  GPU: NVIDIA GPU detected"
    elif check_apple_silicon; then
        is_apple_silicon=true
        echo "  GPU: Apple Silicon detected"
    else
        echo "  GPU: No GPU detected (CPU-only mode)"
    fi
    
    # Set configuration based on detection
    OLLAMA_TARGET_ARCH=$detected_arch
    OLLAMA_TARGET_OS=$detected_os
    
    # Determine profile to use
    if [[ $has_nvidia_gpu == true ]]; then
        OLLAMA_PROFILE="ollama"
        echo -e "${GREEN}Using GPU-accelerated configuration${NC}"
    elif [[ $is_apple_silicon == true ]]; then
        OLLAMA_PROFILE="ollama-arm64"
        echo -e "${GREEN}Using Apple Silicon optimized configuration${NC}"
    else
        OLLAMA_PROFILE="ollama-cpu"
        echo -e "${YELLOW}Using CPU-only configuration${NC}"
    fi
    
    echo ""
}

# Function to create configuration directory
create_config_dir() {
    if [[ ! -d "ollama_config" ]]; then
        mkdir -p ollama_config
        echo -e "${GREEN}Created ollama_config directory${NC}"
    fi
    
    if [[ ! -d "models" ]]; then
        mkdir -p models
        echo -e "${GREEN}Created models directory${NC}"
    fi
}

# Function to generate Ollama configuration file
generate_ollama_config() {
    local config_file="ollama_config/ollama.conf"
    
    cat > "$config_file" << EOF
# Ollama Configuration
# Generated by setup script

# Server configuration
OLLAMA_HOST=0.0.0.0
OLLAMA_ORIGINS=*

# Model configuration
OLLAMA_MODEL=$OLLAMA_MODEL
OLLAMA_TARGET_OS=$OLLAMA_TARGET_OS
OLLAMA_TARGET_ARCH=$OLLAMA_TARGET_ARCH

# Performance configuration
OLLAMA_NUM_THREAD=8

# GPU configuration (if applicable)
$(if [[ $OLLAMA_PROFILE == "ollama" ]]; then
    echo "# NVIDIA GPU enabled"
    echo "OLLAMA_NUM_GPU=1"
fi)
EOF
    
    echo -e "${GREEN}Generated Ollama configuration: $config_file${NC}"
}

# Function to start Ollama with appropriate profile
start_ollama() {
    echo -e "${YELLOW}Starting Ollama with profile: $OLLAMA_PROFILE${NC}"
    
    # Use docker compose if available, otherwise docker-compose
    if docker compose version > /dev/null 2>&1; then
        docker compose -f docker-compose.dev.yml up -d
    else
        docker-compose -f docker-compose.dev.yml up -d
    fi
    
    echo -e "${GREEN}Ollama service started successfully!${NC}"
}

# Function to wait for Ollama to be ready
wait_for_ollama() {
    echo -e "${YELLOW}Waiting for Ollama to be ready...${NC}"
    
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if curl -s "http://localhost:$OLLAMA_PORT/api/tags" > /dev/null 2>&1; then
            echo -e "${GREEN}Ollama is ready!${NC}"
            return 0
        fi
        
        echo -n "."
        sleep 2
        attempt=$((attempt + 1))
    done
    
    echo -e "${RED}Error: Ollama failed to start within the expected time.${NC}"
    return 1
}

# Function to pull the specified model
pull_model() {
    echo -e "${YELLOW}Pulling model: $OLLAMA_MODEL${NC}"
    echo "This may take several minutes depending on your internet connection and model size..."
    
    # Wait a bit more for the service to be fully ready
    sleep 5
    
    if ! curl -X POST "http://localhost:$OLLAMA_PORT/api/pull" \
        -H "Content-Type: application/json" \
        -d "{\"name\": \"$OLLAMA_MODEL\"}" > /dev/null 2>&1; then
        echo -e "${RED}Error: Failed to pull model $OLLAMA_MODEL${NC}"
        return 1
    fi
    
    echo -e "${GREEN}Model $OLLAMA_MODEL pulled successfully!${NC}"
}

# Function to list available models
list_models() {
    echo -e "${YELLOW}Available models:${NC}"
    curl -s "http://localhost:$OLLAMA_PORT/api/tags" | jq -r '.models[] | "  - \(.name) (\(.size | . / 1024 / 1024 / 1024 | round)GB)"' 2>/dev/null || echo "  No models found or jq not available"
}

# Function to show usage information
show_usage() {
    echo -e "${BLUE}Usage:${NC}"
    echo "  $0 [options]"
    echo ""
    echo -e "${BLUE}Options:${NC}"
    echo "  -m, --model MODEL     Specify model to use (default: llama3.1:8b)"
    echo "  -p, --port PORT       Specify port (default: 11434)"
    echo "  -h, --help           Show this help message"
    echo ""
    echo -e "${BLUE}Environment Variables:${NC}"
    echo "  OLLAMA_MODEL         Model to use"
    echo "  OLLAMA_PORT          Port for Ollama service"
    echo "  OLLAMA_TARGET_OS     Target OS (auto-detected)"
    echo "  OLLAMA_TARGET_ARCH   Target architecture (auto-detected)"
    echo ""
    echo -e "${BLUE}Examples:${NC}"
    echo "  $0                                    # Use defaults"
    echo "  $0 -m llama3.1:8b                    # Use specific model"
    echo "  OLLAMA_MODEL=llama3.1:8b $0          # Use environment variable"
}

# Function to parse command line arguments
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -m|--model)
                OLLAMA_MODEL="$2"
                shift 2
                ;;
            -p|--port)
                OLLAMA_PORT="$2"
                shift 2
                ;;
            -h|--help)
                show_usage
                exit 0
                ;;
            *)
                echo -e "${RED}Unknown option: $1${NC}"
                show_usage
                exit 1
                ;;
        esac
    done
}

# Function to check prerequisites
check_prerequisites() {
    echo -e "${BLUE}Checking prerequisites...${NC}"
    
    # Check Docker
    if ! docker info > /dev/null 2>&1; then
        echo -e "${RED}Error: Docker is not running. Please start Docker and try again.${NC}"
        exit 1
    fi
    
    # Check Docker Compose
    if ! command -v docker-compose > /dev/null 2>&1 && ! docker compose version > /dev/null 2>&1; then
        echo -e "${RED}Error: Docker Compose is not available. Please install Docker Compose and try again.${NC}"
        exit 1
    fi
    
    # Check curl
    if ! command -v curl > /dev/null 2>&1; then
        echo -e "${RED}Error: curl is not available. Please install curl and try again.${NC}"
        exit 1
    fi
    
    echo -e "${GREEN}All prerequisites met!${NC}"
    echo ""
}

# Main execution
main() {
    echo -e "${GREEN}=== Advanced Ollama Setup Script ===${NC}"
    echo ""
    
    parse_args "$@"
    
    check_prerequisites
    determine_configuration
    create_config_dir
    generate_ollama_config
    
    echo -e "${BLUE}Configuration Summary:${NC}"
    echo "  Port: $OLLAMA_PORT"
    echo "  Model: $OLLAMA_MODEL"
    echo "  Target OS: $OLLAMA_TARGET_OS"
    echo "  Target Architecture: $OLLAMA_TARGET_ARCH"
    echo "  Profile: $OLLAMA_PROFILE"
    echo ""
    
    start_ollama
    wait_for_ollama
    
    if [ $? -eq 0 ]; then
        pull_model
        echo ""
        list_models
        echo ""
        echo -e "${GREEN}Setup complete! Ollama is running on port $OLLAMA_PORT${NC}"
        echo -e "${YELLOW}You can now use the model: $OLLAMA_MODEL${NC}"
        echo ""
        echo -e "${BLUE}Useful commands:${NC}"
        echo "  docker compose -f docker-compose.dev.yml logs ollama    # View logs"
        echo "  curl http://localhost:$OLLAMA_PORT/api/tags               # List models"
        echo "  docker compose -f docker-compose.dev.yml down          # Stop service"
    else
        echo -e "${RED}Setup failed. Please check the logs with: docker compose -f docker-compose.dev.yml logs${NC}"
        exit 1
    fi
}

# Run main function
main "$@" 