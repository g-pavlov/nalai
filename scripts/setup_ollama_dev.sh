#!/bin/bash

# Development Ollama Setup Script with Efficient Model Path Strategy
# This script implements a strategy to efficiently use Ollama models:
# 1. If OLLAMA_MODELS_PATH env var is provided, use it
# 2. If not, check for well-known paths on the host system
# 3. If none found, use fully containerized approach

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Default configuration
OLLAMA_PORT=${OLLAMA_PORT:-11434}
OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1:8b}
OLLAMA_TARGET_OS=${OLLAMA_TARGET_OS:-linux}

# Well-known Ollama model paths to check
WELL_KNOWN_PATHS=(
    "$HOME/.ollama"
    "/usr/local/share/ollama"
    "/opt/ollama"
    "/var/lib/ollama"
)

# Function to detect system architecture
detect_architecture() {
    local arch=$(uname -m)
    case $arch in
        x86_64|amd64)
            echo "amd64"
            ;;
        aarch64|arm64)
            echo "arm64"
            ;;
        armv7l)
            echo "arm"
            ;;
        *)
            echo "amd64"  # Default fallback
            ;;
    esac
}

# Function to detect OS
detect_os() {
    local os=$(uname -s | tr '[:upper:]' '[:lower:]')
    case $os in
        darwin)
            echo "darwin"
            ;;
        linux)
            echo "linux"
            ;;
        *)
            echo "linux"  # Default fallback
            ;;
    esac
}

# Function to check if a path contains Ollama models
check_ollama_models_path() {
    local path="$1"
    
    if [[ ! -d "$path" ]]; then
        return 1
    fi
    
    # Check for common Ollama model indicators
    if [[ -d "$path/models" ]] || [[ -f "$path/models.json" ]] || [[ -d "$path/blobs" ]]; then
        return 0
    fi
    
    # Check if directory contains model files (common extensions)
    if find "$path" -maxdepth 1 -name "*.bin" -o -name "*.gguf" -o -name "*.safetensors" 2>/dev/null | head -1 | grep -q .; then
        return 0
    fi
    
    return 1
}

# Function to find the best Ollama models path
find_ollama_models_path() {
    echo -e "${BLUE}Searching for existing Ollama models...${NC}" >&2
    
    # First, check if OLLAMA_MODELS_PATH is explicitly set
    if [[ -n "$OLLAMA_MODELS_PATH" ]]; then
        if check_ollama_models_path "$OLLAMA_MODELS_PATH"; then
            echo -e "${GREEN}Using explicit OLLAMA_MODELS_PATH: $OLLAMA_MODELS_PATH${NC}" >&2
            echo "$OLLAMA_MODELS_PATH"
            return 0
        else
            echo -e "${YELLOW}Warning: OLLAMA_MODELS_PATH is set but doesn't contain valid Ollama models: $OLLAMA_MODELS_PATH${NC}" >&2
        fi
    fi
    
    # Check well-known paths
    for path in "${WELL_KNOWN_PATHS[@]}"; do
        if check_ollama_models_path "$path"; then
            echo -e "${GREEN}Found Ollama models in well-known path: $path${NC}" >&2
            echo "$path"
            return 0
        fi
    done
    
    echo -e "${YELLOW}No existing Ollama models found in well-known paths${NC}" >&2
    return 1
}

# Function to create development Docker Compose configuration
create_dev_compose_config() {
    local models_path="$1"
    local compose_file="docker-compose.dev.yml"
    
    echo -e "${BLUE}Creating development Docker Compose configuration...${NC}" >&2
    
    # Detect system configuration
    local detected_arch=$(detect_architecture)
    local detected_os=$(detect_os)
    
    cat > "$compose_file" << EOF
# Development Docker Compose Configuration
# Generated by setup_ollama_dev.sh

services:
  api-assistant:
    build:
      context: .
      dockerfile: Dockerfile.dev
    ports:
      - "8080:8080"
    volumes:
      # Mount source code for development
      - ./src/api_assistant:/app/api_assistant
      # Mount logs directory for persistence
      - ./logs:/app/logs
      # Mount API specs and guidelines
      - ./data:/app/data
    env_file:
      - .env
    environment:
      - MODEL_ID=${OLLAMA_MODEL}
      - MODEL_PLATFORM=ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - ECOMMERCE_API_URL=http://ecommerce-mock:8000
      - ENABLE_API_CALLS=true
      - API_CALLS_BASE_URL=http://ecommerce-mock:8000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      - ollama
      - ecommerce-mock

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-dev
    ports:
      - "\${OLLAMA_PORT:-11434}:11434"
    volumes:
EOF
    
    # Add model path volume mount if found
    if [[ -n "$models_path" ]]; then
        echo "      - ${models_path}:/root/.ollama" >> "$compose_file"
        echo -e "${GREEN}  ✓ Mounting existing models from: $models_path${NC}" >&2
    else
        echo "      - ollama_data:/root/.ollama" >> "$compose_file"
        echo -e "${YELLOW}  ✓ Using containerized models (no existing models found)${NC}" >&2
    fi
    
    cat >> "$compose_file" << EOF
      - ./models:/models
      - ./ollama_config:/etc/ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - OLLAMA_TARGET_OS=${detected_os}
      - OLLAMA_TARGET_ARCH=${detected_arch}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  ecommerce-mock:
    build:
      context: .
      dockerfile: Dockerfile.ecommerce
    container_name: ecommerce-mock-dev
    ports:
      - "8000:8000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

EOF
    
    # Add volumes section only if not using external models
    if [[ -z "$models_path" ]]; then
        cat >> "$compose_file" << EOF
volumes:
  ollama_data:
    driver: local
EOF
    fi
    
    echo -e "${GREEN}Created development configuration: $compose_file${NC}" >&2
}

# Function to check if Docker is running
check_docker() {
    if ! docker info > /dev/null 2>&1; then
        echo -e "${RED}Error: Docker is not running. Please start Docker and try again.${NC}"
        exit 1
    fi
}

# Function to check if Docker Compose is available
check_docker_compose() {
    if ! command -v docker-compose > /dev/null 2>&1 && ! docker compose version > /dev/null 2>&1; then
        echo -e "${RED}Error: Docker Compose is not available. Please install Docker Compose and try again.${NC}"
        exit 1
    fi
}

# Function to start development services
start_dev_services() {
    echo -e "${YELLOW}Starting development services...${NC}"
    
    # Use docker compose if available, otherwise docker-compose
    if docker compose version > /dev/null 2>&1; then
        docker compose -f docker-compose.dev.yml up -d
    else
        docker-compose -f docker-compose.dev.yml up -d
    fi
    
    echo -e "${GREEN}Development services started successfully!${NC}"
}

# Function to wait for Ollama to be ready
wait_for_ollama() {
    echo -e "${YELLOW}Waiting for Ollama to be ready...${NC}"
    
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if curl -s "http://localhost:$OLLAMA_PORT/api/tags" > /dev/null 2>&1; then
            echo -e "${GREEN}Ollama is ready!${NC}"
            return 0
        fi
        
        echo -n "."
        sleep 2
        attempt=$((attempt + 1))
    done
    
    echo -e "${RED}Error: Ollama failed to start within the expected time.${NC}"
    return 1
}

# Function to check if model is available
check_model_availability() {
    echo -e "${YELLOW}Checking model availability: $OLLAMA_MODEL${NC}"
    
    local models_response=$(curl -s "http://localhost:$OLLAMA_PORT/api/tags" 2>/dev/null || echo "")
    
    if echo "$models_response" | jq -e ".models[] | select(.name == \"$OLLAMA_MODEL\")" > /dev/null 2>&1; then
        echo -e "${GREEN}✓ Model $OLLAMA_MODEL is available${NC}"
        return 0
    else
        echo -e "${YELLOW}⚠ Model $OLLAMA_MODEL is not available${NC}"
        return 1
    fi
}

# Function to pull the specified model if needed
pull_model_if_needed() {
    if ! check_model_availability; then
        echo -e "${YELLOW}Pulling model: $OLLAMA_MODEL${NC}"
        echo "This may take several minutes depending on your internet connection and model size..."
        
        if ! curl -X POST "http://localhost:$OLLAMA_PORT/api/pull" \
            -H "Content-Type: application/json" \
            -d "{\"name\": \"$OLLAMA_MODEL\"}" > /dev/null 2>&1; then
            echo -e "${RED}Error: Failed to pull model $OLLAMA_MODEL${NC}"
            return 1
        fi
        
        echo -e "${GREEN}Model $OLLAMA_MODEL pulled successfully!${NC}"
    fi
}

# Function to list available models
list_models() {
    echo -e "${YELLOW}Available models:${NC}"
    curl -s "http://localhost:$OLLAMA_PORT/api/tags" | jq -r '.models[] | "  - \(.name) (\(.size | . / 1024 / 1024 / 1024 | round)GB)"' 2>/dev/null || echo "  No models found or jq not available"
}

# Function to create configuration directory
create_config_dir() {
    if [[ ! -d "ollama_config" ]]; then
        mkdir -p ollama_config
        echo -e "${GREEN}Created ollama_config directory${NC}"
    fi
    
    if [[ ! -d "models" ]]; then
        mkdir -p models
        echo -e "${GREEN}Created models directory${NC}"
    fi
}

# Main execution
main() {
    echo -e "${GREEN}=== Development Ollama Setup with Efficient Model Path Strategy ===${NC}"
    echo ""
    
    check_docker
    check_docker_compose
    create_config_dir
    
    # Find the best Ollama models path
    local models_path=$(find_ollama_models_path)
    
    # Create development Docker Compose configuration
    create_dev_compose_config "$models_path"
    
    # Start services
    start_dev_services
    wait_for_ollama
    
    if [ $? -eq 0 ]; then
        pull_model_if_needed
        echo ""
        list_models
        echo ""
        echo -e "${GREEN}Development setup complete!${NC}"
        echo -e "${BLUE}Services running:${NC}"
        echo "  - API Assistant: http://localhost:8080"
        echo "  - Ollama: http://localhost:$OLLAMA_PORT"
        echo "  - Ecommerce Mock: http://localhost:8000"
        echo ""
        if [[ -n "$models_path" ]]; then
            echo -e "${GREEN}✓ Using existing models from: $models_path${NC}"
        else
            echo -e "${YELLOW}✓ Using containerized models (no existing models found)${NC}"
        fi
        echo -e "${YELLOW}Model: $OLLAMA_MODEL${NC}"
    else
        echo -e "${RED}Setup failed. Please check the logs with: docker compose -f docker-compose.dev.yml logs ollama${NC}"
        exit 1
    fi
}

# Run main function
main "$@" 